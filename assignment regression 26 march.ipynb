{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4566609-a954-4cdc-8e9b-f0e1ffc32a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS1\n",
    "simple linear regression is used to train machine models having single independent and dependent features whereas multiple linear\n",
    "regression posses more than one independent features. \n",
    "for example if we have to predict the price of house, it only depend on no. of rooms in case of linear regression whereas in multiple linear regression\n",
    "the price of house will depend on multiple factors such as no. rooms, size of house, location of house, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cfbbf-b013-4a25-be93-bff3a2b81dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS2\n",
    "The main assumptions of linear regression are: linear relationship,normal distribution of residuals,multicollinearity,autocorrelation.\n",
    "1. we always assume that there is always linear relationship between dependent and independent varible.\n",
    "2. all residulas or error terms should be normally distributed\n",
    "3. there should be no multicollinearity in our dataset.\n",
    "4. there should not be autocorrelaion between given dataset.\n",
    "\n",
    "graphical tools like scatter plots, residual plots, and normality plots can provide visual insights into the assumptions.\n",
    "and other than this there are other methods also residual analysis, collinearity assessment etc.\n",
    "Residual analysis: Examine the residuals of the model to check for patterns or systematic deviations from randomness.\n",
    "Collinearity assessment: Calculate correlation coefficients between predictor variables to identify highly correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1191fd8-b101-49a5-ab19-d2f74e84704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS3\n",
    "In a linear regression model,the slope and intercept represent the relationship between the independent variable\n",
    "(predictor) and the dependent variable (target). \n",
    "\n",
    "Slope:it represents the change in the target variable associated with a one-unit change in the predictor variable, \n",
    "holding all other variables constant. It indicates the rate of change in the target variable for each unit increase in the\n",
    "predictor variable. A positive slope indicates a positive relationship, while a negative slope indicates a negative relationship.\n",
    "\n",
    "Intercept: It represents the predicted value of the target variable when all predictor variables are zero.\n",
    "It is the point where the regression line intersects the y-axis.\n",
    "The intercept provides the baseline or starting point for the target variable before considering the effect of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc2931-e256-468d-96f6-562ece9be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS4\n",
    "Gradient descent is an optimization algorithm that is used to find the minimum of cost function. \n",
    "gradient descent starts with an initial value of intercept and slope, and iteratively updates them to minimize the cost function.\n",
    "The goal is to find slope and intercept values that yield the lowest cost function.\n",
    "At each iteration, gradient descent calculates the gradient of the cost function with slope. \n",
    "The gradient is a vector that points in the direction of the steepest ascent of the function.\n",
    "The iterations continue until a stopping criterion is met.\n",
    "This criterion can be a maximum number of iterations, a small change in the cost function, or \n",
    "reaching a predefined threshold of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7f503-84ca-43ca-83f8-216c17951e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS5\n",
    "Multiple linear regression is an extension of simple linear regression that allows for the modeling of the relationship between a dependent variable and multiple independent variables. \n",
    "It is used when there are more than one independent varaibles or features.\n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ  \n",
    "this is the equation for multiple linear regression.\n",
    "Number of independent varaibles: Simple linear regression involves only one independent varaible, while multiple linear regression \n",
    "involves two or more independent varaibles.\n",
    "Equation: The equation for simple linear regression is Y = β₀ + β₁X, whereas the equation for multiple linear regressin is \n",
    "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ \n",
    "Model complexity: Multiple linear regression allows for more flexibility and complexity in modeling the relationships between variables.\n",
    " while simple linear regression focuses on the relationship between one predictor and the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca7cf1c-6a7a-4bef-b134-c9f3e8673b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS6\n",
    "Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly\n",
    "correlated with each other. It indicates a strong linear relationship between the independent variables, \n",
    "which can cause issues in the regression analysis. Multicollinearity can affect the interpretation of\n",
    "coefficients, stability of the model, and lead to unreliable or misleading results.\n",
    "\n",
    "detect\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of predictor variables.\n",
    "A correlation coefficient close to +1 or -1 indicates high collinearity.\n",
    "\n",
    "Variance Inflation Factor (VIF): Compute the VIF for each predictor variable.\n",
    "VIF measures how much the variance of a coefficient is inflated due to multicollinearity. \n",
    "\n",
    "Address multicollinearity:\n",
    "\n",
    "Remove redundant predictors: If two or more predictors are highly correlated, consider removing one of them from the model.\n",
    "Feature selection: Use techniques such as stepwise regression or regularization methods.\n",
    "Data collection: Collecting more data can help alleviate multicollinearity by introducing more variability and reducing the correlation.\n",
    "Transform variables: If the relationship between predictors is non-linear, transforming variables (e.g., taking logarithms, square roots) can reduce multicollinearity.\n",
    "Ridge regression: Applying Ridge regression, which adds a penalty term to the regression objective function, \n",
    "can help mitigate multicollinearity by shrinking the coefficients and reducing their sensitivity to collinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f987033-06bc-4041-8096-552991af6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS7\n",
    "Polynomial regression is an extension of linear regression that allows for modeling nonlinear\n",
    "relationships between the dependent variable and the independent variable(s). While linear regression assumes a \n",
    "linear relationship between variables, polynomial regression captures more complex patterns.\n",
    "\n",
    "Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ \n",
    "\n",
    "Linearity assumption: Linear regression assumes a linear relationship between the dependent and independent variables,\n",
    "Additional predictors: In polynomial regression, additional predictors are introduced by raising the original independent \n",
    "variable to higher powers (e.g., X², X³, etc.). These polynomial terms provide flexibility to model nonlinear relationships \n",
    "and capture more complex patterns in the data.\n",
    "Model complexity: Polynomial regression can produce more flexible and complex models compared to linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c358f-617e-41b8-ab64-a19e80d644fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS8\n",
    "Advantages of polynomial relationship:\n",
    "    \n",
    "Captures nonlinear relationships: Polynomial regression can model nonlinear relationships between variables, allowing for more flexibility in capturing complex patterns in the data. \n",
    "Higher order interactions: Polynomial regression can account for higher order interactions among variables.\n",
    "By including polynomial terms, it can capture interactions that linear regression cannot, leading to a more accurate \n",
    "representation of the data.\n",
    "Improved fit and predictive power: When the relationship between variables is nonlinear, polynomial regressio\n",
    "n can provide a better fit to the data compared to linear regression. It can yield improved predictive power by accounting\n",
    "for more complex \n",
    "\n",
    "Disadvantes \n",
    "Increased model complexity: As the degree of the polynomial increases, the complexity of the model also increases.\n",
    "Interpretation challenges: Polynomial regression can be more challenging to interpret compared to linear regression. \n",
    "Nonlinear relationships: When there is evidence of a nonlinear relationship between the dependent and independent variables,\n",
    "polynomial regression can be used.\n",
    "Higher order interactions: If there are indications that interactions among variables are important and \n",
    "cannot be adequately captured by linear regression, polynomial regression can be a suitable choice. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

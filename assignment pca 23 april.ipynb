{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64d7f-93cb-4090-bf52-66de527129bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS1\n",
    "curse of dimensionality reduction- it is decrease in accuracy as we keep on incresing the less important features in our\n",
    "machine learning model. it is necessary to reduce it because to improve accuracy and performance.\n",
    "addtion of unimportant features leads to increase in overall volume of data which generate so many problesms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27f2b4-8bc7-4b42-b276-74358bdd60e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS2\n",
    "Increased computational complexity: As the number of dimensions (features) increases, the computational requirements\n",
    "of machine learning algorithms grow exponentially.\n",
    "Sparsity of data: In high-dimensional spaces, data points become sparsely distributed.\n",
    "As the number of dimensions increases, the available data becomes insufficient to cover the entire feature space adequately. \n",
    "Increased risk of overfitting: High-dimensional data provides more degrees of freedom for models, increasing the risk of overfitting.\n",
    "hence it decreases overall performance of mahine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77fbea-9c41-4cff-abb3-286e8edee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS3\n",
    "Following are the consequences of curse of dimesinality:\n",
    "Increased computational complexity: As the number of dimensions (features) increases, the computational requirements\n",
    "of machine learning algorithms grow exponentially.\n",
    "Sparsity of data: In high-dimensional spaces, data points become sparsely distributed.\n",
    "As the number of dimensions increases, the available data becomes insufficient to cover the entire feature space adequately. \n",
    "Increased risk of overfitting: High-dimensional data provides more degrees of freedom for models, increasing the risk of overfitting.\n",
    "hence it decreases overall performance of mahine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd29b0-89be-4c0d-87c5-3152681bb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS4\n",
    "feature selection is a technique of selecting most important features out of given data. by this tehnique we remove \n",
    "unimportant or redudant features and hence reduce the dimensions.\n",
    "it helps machine learning model to improve ovrall performance.\n",
    "it enhance interpretability- like it will be easier for model to analyse the pattern.\n",
    "reduce computational complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53159b2-3e92-4f13-8e77-ac950f8214ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS5\n",
    "loss: Dimensionality reduction techniques,can result in information loss. \n",
    "By reducing the dimensionality, some of the original information and variance in the data may be discarded or compressed\n",
    "Sensitivity to outliers and noise: Dimensionality reduction techniques can be sensitive to outliers or noisy data points.\n",
    "interpretability- It can be challenging to relate the reduced dimensions back to the original features, making it harder to\n",
    "interpret the underlying meaning of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b16f1d-3e28-4b86-be32-aa6eae4bd0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS6\n",
    "Curse of dimesnsionality increases the risk of underfitting and overfitting both.\n",
    "for overfitting as we have already discussed that large dimensions data provide more degree of freedom and hnece increases flexibility \n",
    "for the patterns present in our data and it leads to introduction of noise in our data and hence all these features contribute to \n",
    "overfitting \n",
    "now for underfitting- it occurs when our data is too simple and it unables to caputure the underlying pattern in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ed667-e95e-4266-91b1-ab69515fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS7\n",
    "Variance explained: For techniques like Principal Component Analysis (PCA), we can analyze the cumulative explained variance ratio. \n",
    "The explained variance represents the amount of information retained by each principal component. now we can check that how many \n",
    "principal components are retaining most of the informations.\n",
    "\n",
    "Scree plot or eigenvalues: In PCA, we can plot the eigenvalues or the amount of variance explained by each principal component\n",
    "in descending order. here we plot a graph in decreasing order of eigen values.thn at layoff point eigen value will drop suddlenl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
